{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Here I will document my method for cleaning my data files, which were taken from Kaggle's dataset “Comments on articles published in the New York Times” (https://www.kaggle.com/aashita/nyt-comments).\n",
    "\n",
    "This code is broken into two sections - one cleans the data files with articles and one cleans the data files with comments. Here I test my cleaning using only a single file, but by concatenating data files together, I can run this code on the entire dataset.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-452443b3fa9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "sent_token = nltk.sent_tokenize\n",
    "import csv  \n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#For all data - train = pd.read_csv(\"/root/Springboard/Data/cleaning/allArticles.csv\")\n",
    "train = pd.read_csv(\"/root/Springboard/Data/ArticlesApril2017.csv\")\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Initial goals: \n",
    "\n",
    "-Make sure the contents of each field are the correct type and have no missing data (i.e. scrub the 'NaN' from the 'abstract' field)\n",
    "\n",
    "-Make sure that the data comes properly tokenized\n",
    "\n",
    "-Convert all words to lowercase (to avoid confusion between uppercase and lowercase versions of the same word)\n",
    "\n",
    "Several of these data columns (articleID, articleWordCount, multimedia, printPage) contain only integers or single lowercase words.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for column in train:\n",
    "    #print(train[column].get_dtype_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>From the above code, the only integer columns are 2, 7 and 9. The rest are string columns and need to be converted to lowercase. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize = 4\n",
    "train = pd.read_csv(\"/root/Springboard/Data/ArticlesApril2017.csv\", header=0, nrows=sampleSize)\n",
    "#train.head(5)\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "nonstrings = [2, 7, 9]\n",
    "\n",
    "\n",
    "train= train.astype(str)\n",
    "train.fillna(0)\n",
    "\n",
    "def clean_articles(doc):\n",
    "    for index, column in enumerate(doc):\n",
    "        if index in nonstrings:\n",
    "            doc[column] = doc[column].astype(str)\n",
    "            continue\n",
    "        doc[column] = doc[column].str.replace('[^\\w\\s]','')\n",
    "        doc[column] = doc[column].str.lower()\n",
    "        #doc[column] = doc[column].str.strip()\n",
    "        doc[column] = doc[column].replace(np.nan, '', regex=True)\n",
    "        doc[column].apply(nltk.word_tokenize)\n",
    "        doc[column].apply(lemmatize_text)\n",
    "        doc[column] = [token for token in doc[column] if token not in stop_words]\n",
    "    return doc\n",
    "\n",
    "def lemmatize_text(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "#For testing, but this functionality has been compressed into clean_articles\n",
    "def norm_articles(doc):\n",
    "    clean_articles(doc)\n",
    "    doc3 = []\n",
    "    for index, row in doc.iterrows():\n",
    "        element = str(row.to_frame().T)\n",
    "        print(index)\n",
    "        tokenData = nltk.word_tokenize(element)\n",
    "        lemma_tokens = lemmatize_text(tokenData)\n",
    "        #print(tokenData)\n",
    "        #print(\"EFFEW\")\n",
    "        filtered_tokens = [token for token in lemma_tokens if token not in stop_words]\n",
    "        doc2 = ' '.join(filtered_tokens)\n",
    "        doc3.append(doc2)\n",
    "    #print(type(doc3))\n",
    "    return doc3\n",
    "\n",
    "\n",
    "\n",
    "normalize_corpus = np.vectorize(norm_articles)\n",
    "\n",
    "clean_art = clean_articles(train)\n",
    "\n",
    "clean_art.head(5)\n",
    "#pprint(clean_art2)\n",
    "\n",
    "#print('\\n'.join(clean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Now we want a separate section to clean the comment files. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all data - train = pd.read_csv(\"/root/Springboard/Data/cleaning/allComments.csv\")\n",
    "\n",
    "sampleSize = 1000\n",
    "train = pd.read_csv(\"/root/Springboard/Data/CommentsApril2017.csv\", nrows=sampleSize)\n",
    "\n",
    "train= train.astype(str)\n",
    "train.fillna(0)\n",
    "strings = [1, 5, 10, 18, 22, 24, 25, 26, 29, 30, 33]\n",
    "\n",
    "def lemmatize_text(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "def clean_articles(doc):\n",
    "    for index, column in enumerate(doc):\n",
    "        if index in strings:         \n",
    "            doc[column] = doc[column].str.replace('[^\\w\\s]','')\n",
    "            doc[column] = doc[column].str.lower()\n",
    "            #doc[column] = doc[column].str.strip()\n",
    "            doc[column] = doc[column].replace(np.nan, '', regex=True)\n",
    "            doc[column].apply(nltk.word_tokenize)\n",
    "            doc[column].apply(lemmatize_text)\n",
    "            doc[column].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "        else:\n",
    "            doc[column] = doc[column].astype(str)\n",
    "            continue\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(norm_articles)\n",
    "\n",
    "clean_comments = clean_articles(train)\n",
    "#The second command takes awhile to run\n",
    "clean_comments.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_file_name = \"cleaned_article_data.csv\"\n",
    "clean_art_csv = clean_art.to_csv(art_file_name, encoding='utf-8', index=False)\n",
    "\n",
    "com_file_name = \"cleaned_comment_data.csv\"\n",
    "clean_com_csv = clean_comments.to_csv(com_file_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Here I have saved the cleaned data files to storage. This is a good break point where I can use these files to begin looking at possible ML models. \n",
    "    \n",
    "   In the next section, I will explore the data, looking at features and ways to visualize the feature set. I begin by re-importing the libraries that I will use. That way, this section of the notebook can be reran separately later. I will load the cleaned files in so that I know I am not making any changes to the stored version.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "ntlk.download()\n",
    "import re\n",
    "sent_token = nltk.sent_tokenize\n",
    "import csv  \n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "file_path_art = '/root/Springboard/Data/cleaned_article_data.csv'\n",
    "clean_art = pd.read_csv(file_path_art, index_col = False)\n",
    "clean_art\n",
    "\n",
    "file_path_comments = '/root/Springboard/Data/cleaned_comment_data.csv'\n",
    "clean_comments = pd.read_csv(file_path_comments, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> In the above cell, I loaded in the cleaned article data. Here I load in the cleaned\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "features = clean_comments.columns.tolist()\n",
    "output = 'recommendations'\n",
    "features.remove('recommendations')\n",
    "\n",
    "for column in clean_comments.columns:\n",
    "    clean_comments[column] = clean_comments[column].astype(str)\n",
    "    if clean_comments[column].dtype == type(object):\n",
    "        clean_comments[column] = le.fit_transform(clean_comments[column])\n",
    "\n",
    "#print(features)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "clean_comments.astype(int)\n",
    "clean_comments.head(5)\n",
    "\n",
    "#clean_comments.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "param = {'n_estimators': [5,10,100],\n",
    "         'max_depth': [10, 30, 100] }\n",
    "\n",
    "gs=GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(clean_comments[features], clean_comments[output])\n",
    "pd.DataFrame(gs_fit.cv_results_).head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
