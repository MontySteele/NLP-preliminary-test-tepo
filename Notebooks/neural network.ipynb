{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook I will be trying to fit my data to a neural network. I will be following some example models for applying deep learning to NLP datasets as described at https://nlpforhackers.io/keras-intro/\n",
    "\n",
    "First I will load my cleaned dataset and perform a train-test split, then vectorize my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "sent_token = nltk.sent_tokenize\n",
    "import csv  \n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "\n",
    "file_path_comments = r'~/Documents/Springboard/Springboard/Data/cleaned_comment_data.csv'\n",
    "\n",
    "#file_path_comments = r'/mnt/c/Users/msteele9/Documents/Springboard/Springboard/Data/cleaned_comment_data.csv'\n",
    "clean_comments = pd.read_csv(file_path_comments, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This project makes me happy to be a 30+ year T...\n",
       "1    Stunning photos and reportage. Infuriating tha...\n",
       "2    Brilliant work from conception to execution. I...\n",
       "3    NYT reporters should provide a contributor's l...\n",
       "4       Could only have been done in print. Stunning. \n",
       "Name: commentBody, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_comments['commentBody'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "X = clean_comments['commentBody']\n",
    "y = clean_comments['recommendations']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random.seed(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000,)\n",
      "(8000,)\n",
      "(32000,)\n",
      "(8000,)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(len(X_train) == len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "br           12645\n",
       "trump         8269\n",
       "people        6197\n",
       "one           5228\n",
       "would         5180\n",
       "like          4800\n",
       "us            3827\n",
       "get           3501\n",
       "even          3152\n",
       "many          3146\n",
       "time          3075\n",
       "think         2924\n",
       "way           2707\n",
       "president     2702\n",
       "world         2637\n",
       "good          2563\n",
       "could         2549\n",
       "well          2476\n",
       "know          2445\n",
       "see           2440\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "\n",
    "# Setting the vectorizer just like we would set a model \n",
    "cvec = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n",
    "                             lowercase=True, max_features=5000)\n",
    "# Fitting the vectorizer on our training data \n",
    "cvec.fit(X_train)\n",
    "\n",
    "X_train_df = pd.DataFrame(cvec.transform(X_train).todense(),\n",
    "                       columns=cvec.get_feature_names())\n",
    "\n",
    "X_test_df = pd.DataFrame(cvec.transform(X_test).todense(),\n",
    "                      columns=cvec.get_feature_names())\n",
    "\n",
    "word_counts = X_train_df.sum(axis=0)\n",
    "word_counts.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\msteele9\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\msteele9\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\msteele9\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\msteele9\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\msteele9\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\msteele9\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               2500500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 2,501,001\n",
      "Trainable params: 2,501,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 5} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    " \n",
    "model = Sequential()\n",
    " \n",
    "model.add(Dense(units=500, activation='relu', input_dim=len(cvec.get_feature_names())))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 32000 samples\n",
      "Epoch 1/3\n",
      "32000/32000 [==============================] - 4s 123us/step - loss: -2560.4494 - acc: 5.6250e-04 - val_loss: -2855.9507 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "32000/32000 [==============================] - 3s 102us/step - loss: -2862.3845 - acc: 3.1250e-05 - val_loss: -2866.5305 - val_acc: 2.5000e-04\n",
      "Epoch 3/3\n",
      "32000/32000 [==============================] - 3s 103us/step - loss: -2867.2161 - acc: 7.5000e-04 - val_loss: -2867.6740 - val_acc: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x210b9ce56d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_df, y_train, \n",
    "          epochs=3, batch_size=100, verbose=1, \n",
    "          validation_data=(X_train_df, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step\n",
      "Accuracy: 0.001375\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(cvec.transform(X_test), y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1])  # Accuracy: 0.875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2318, 4513]\n",
      "[3080, 4683, 3779, 4231, 1742, 1842, 2681, 2211, 12, 599, 599, 2062]\n"
     ]
    }
   ],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(cvec.get_feature_names())}\n",
    "tokenize = cvec.build_tokenizer()\n",
    "preprocess = cvec.build_preprocessor()\n",
    " \n",
    "def to_sequence(tokenizer, preprocessor, index, text):\n",
    "    words = tokenizer(preprocessor(text))\n",
    "    indexes = [index[word] for word in words if word in index]\n",
    "    return indexes\n",
    " \n",
    "print(to_sequence(tokenize, preprocess, word2idx, \"This is an important test!\"))  # [2269, 4453]\n",
    "X_train_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in X_train]\n",
    "print(X_train_sequences[0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SEQ_LENGHT= 236\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000\n",
      " 3080 4683 3779 4231 1742 1842 2681 2211   12  599  599 2062]\n"
     ]
    }
   ],
   "source": [
    "# Compute the max lenght of a text\n",
    "MAX_SEQ_LENGHT = len(max(X_train_sequences, key=len))\n",
    "print(\"MAX_SEQ_LENGHT=\", MAX_SEQ_LENGHT)\n",
    " \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "N_FEATURES = len(cvec.get_feature_names())\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
    "print(X_train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 236, 64)           320064    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 232, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 46, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2944)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                188480    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 529,153\n",
      "Trainable params: 529,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(len(cvec.get_feature_names()) + 1,\n",
    "                    64,  # Embedding size\n",
    "                    input_length=MAX_SEQ_LENGHT))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31900 samples, validate on 100 samples\n",
      "Epoch 1/3\n",
      "31900/31900 [==============================] - 2s 71us/step - loss: -2394.7012 - acc: 1.8809e-04 - val_loss: -1804.6780 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "31900/31900 [==============================] - 1s 17us/step - loss: -2871.2585 - acc: 0.0000e+00 - val_loss: -1804.6780 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "31900/31900 [==============================] - 1s 17us/step - loss: -2871.2585 - acc: 0.0000e+00 - val_loss: -1804.6780 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x210b9e1aa90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_sequences[:-100], y_train[:-100], \n",
    "          epochs=3, batch_size=512, verbose=1,\n",
    "          validation_data=(X_train_sequences[-100:], y_train[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in X_test]\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test_sequences, y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1]) # 0.8766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 236, 64)           320064    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 353,153\n",
      "Trainable params: 353,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(len(cvec.get_feature_names()) + 1,\n",
    "                    64,  # Embedding size\n",
    "                    input_length=MAX_SEQ_LENGHT))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31900 samples, validate on 100 samples\n",
      "Epoch 1/2\n",
      "31900/31900 [==============================] - 81s 3ms/step - loss: -2443.1617 - acc: 0.0010 - val_loss: -1804.6780 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "31900/31900 [==============================] - 83s 3ms/step - loss: -2871.2585 - acc: 0.0000e+00 - val_loss: -1804.6780 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211471a97b8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_sequences[:-100], y_train[:-100], \n",
    "          epochs=2, batch_size=128, verbose=1, \n",
    "          validation_data=(X_train_sequences[-100:], y_train[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 29s 4ms/step\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test_sequences, y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1]) # 0.875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-5fab3fec724f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_md'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mEMBEDDINGS_LEN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'apple'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_md')\n",
    " \n",
    "EMBEDDINGS_LEN = len(nlp.vocab['apple'].vector)\n",
    "print(\"EMBEDDINGS_LEN=\", EMBEDDINGS_LEN)  # 300\n",
    " \n",
    "embeddings_index = np.zeros((len(cvec.get_feature_names()) + 1, EMBEDDINGS_LEN))\n",
    "for word, idx in word2idx.items():\n",
    "    try:\n",
    "        embedding = nlp.vocab[word].vector\n",
    "        embeddings_index[idx] = embedding\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(len(cvec.get_feature_names()) + 1,\n",
    "                    EMBEDDINGS_LEN,  # Embedding size\n",
    "                    weights=[embeddings_index],\n",
    "                    input_length=MAX_SEQ_LENGHT,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_sequences[:-100], y_train[:-100], \n",
    "          epochs=1, batch_size=128, verbose=1, \n",
    "          validation_data=(X_train_sequences[-100:], y_train[-100:]))\n",
    " \n",
    "scores = model.evaluate(X_test_sequences, y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1])  # 0.8508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "# Shuffle the data and then split it, keeping 20% aside for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    " \n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "vectorizer.fit(X_train)\n",
    " \n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100,))\n",
    "classifier.fit(vectorizer.transform(X_train), y_train)\n",
    " \n",
    "print(\"Score:\", classifier.score(vectorizer.transform(X_test), y_test))  # Score: 0.8816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    " \n",
    " \n",
    "class SimpleNeuralNetwork(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layer_size=100, learning_rate=.1, epochs=1000, debug_print_epoch=10):\n",
    "        assert hidden_layer_size > 0\n",
    "        self.hidden_layer_size_ = hidden_layer_size\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.epochs_ = epochs\n",
    "        self.debug_print_epoch_ = debug_print_epoch\n",
    " \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)  # Makes sure the X and y play nice\n",
    " \n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        # In this particular case, we'll make sure the number of classes is 2\n",
    "        assert n_classes == 2\n",
    " \n",
    "        n_samples, n_features = X.shape\n",
    " \n",
    "        self.binarizer_ = LabelBinarizer().fit(y)\n",
    "        Y_binary = self.binarizer_.transform(y)\n",
    " \n",
    "        # Compute the weight matrices sizes and init with small random values\n",
    " \n",
    "        # Hidden Layer\n",
    "        self.A1_ = np.random.randn(n_features, self.hidden_layer_size_)\n",
    "        # Output Layer\n",
    "        self.A2_ = np.random.randn(self.hidden_layer_size_, 1)\n",
    " \n",
    "        # ~~ SKIP TRAINING FOR NOW ~~\n",
    " \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Output probabilities for each sample\"\"\"\n",
    "        # make sure X is of an accepted type\n",
    "        X = check_array(X, accept_sparse='csr')  \n",
    " \n",
    "        # Apply linear function at the hidden layer\n",
    "        Y_hidden = X.dot(self.A1_)\n",
    " \n",
    "        # Apply sigmoid at the output layer\n",
    "        Y_output = sigmoid(Y_hidden.dot(self.A2_))\n",
    " \n",
    "        return np.hstack((1 - Y_output, Y_output))\n",
    " \n",
    "    def predict(self, X):\n",
    "        \"\"\" Output only the most likely class for each sample \"\"\"\n",
    "        scores = self.predict_proba(X)\n",
    "        indices = scores.argmax(axis=1)\n",
    "        return self.binarizer_.inverse_transform(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "# Shuffle the data and then split it, keeping 20% aside for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "vectorizer = CountVectorizer(lowercase=True, binary=True)\n",
    "vectorizer.fit(X_train)\n",
    " \n",
    "classifier = SimpleNeuralNetwork(hidden_layer_size=100, epochs=500, learning_rate=0.1)\n",
    "classifier.fit(vectorizer.transform(X_train), list(y_train.values))\n",
    " \n",
    "print(\"Score:\", classifier.score(vectorizer.transform(X_test), y_test))  # 0.5056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
